{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hhHyCOZglf7m"
   },
   "source": [
    "## 1. Bellman Optimality Equations\n",
    "$\n",
    "\\begin{align}\n",
    "\\epsilon_{k+1}&= \\quad || v_{k+1}(s)-v_{*}(s)||_{\\infty}\\quad\\\\\n",
    "&= \\quad \\max\\limits_s(\\vert\\max\\limits_a(\\sum\\limits_{s',r}P(s',r|s,a)[r+\\gamma\\cdot v_{k}(s')] - \\max\\limits_a'(\\sum\\limits_{s',r}P(s',r|s,a')[r+\\gamma\\cdot v_{*}(s')]\\vert)\\\\\n",
    "&= \\quad \\max\\limits_s(\\vert\\max\\limits_{a}(\\sum\\limits_{s',r}P(s',r|s,a) \\cdot \\gamma \\cdot[v_k(s')-v_*(s')])\\vert)\\\\\n",
    "&\\leq \\quad \\gamma \\cdot\\vert \\max_{s'}[v_k(s')-v_*(s')] \\vert\\\\\n",
    "&= \\quad\\gamma\\cdot||v_k(s)-v_*(s)||_{\\infty}\\\\\n",
    "&= \\quad \\gamma \\cdot \\epsilon_{k} \\quad \\gamma \\in (0,1), k=1...T\\\\\n",
    "&s.t. \\quad \\epsilon_{k+1} \\leq \\gamma^k \\epsilon_1\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zooOwVfJ8vQ3"
   },
   "source": [
    "Therefore Bellman optimality equation converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaN7VcXxpVGo"
   },
   "source": [
    "## 2. Policy Iteration\n",
    "## 2.1 Policy Improvement Guarantees\n",
    "Let's say we have an old policy $\\pi$ and a changed policy $\\pi'$. \n",
    "\n",
    "In policy improvement phase, $\\pi'(s)=\\max\\limits_aq_\\pi(s,a)$, for $s \\in S$.\n",
    "\n",
    "So, $v_\\pi(s) \\leq q_\\pi(s,\\pi'(s))$, that is to say, the old policy can be improved by choosing a new immediate action denoted as $\\pi'(s)$ and then follows itself again.\n",
    "\n",
    "Therefore it follows that:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "v_\\pi(s) \\quad &\\leq \\quad q_\\pi(s,\\pi'(s))\\\\\n",
    "&= \\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+\\gamma\\cdot v_\\pi(S_{t+1})\\mid S_t=s]\\\\\n",
    "&\\leq \\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+ \\gamma\\cdot q_\\pi(S_{t+1},\\pi'(S_{t+1}))\\mid S_t=s]\\\\\n",
    "&= \\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+\\gamma\\cdot\\mathop{\\mathbb{E}_{\\pi'}}[R_{t+2}+\\gamma\\cdot v_\\pi(S_{t+2})\\mid S_t=s]\\\\\n",
    "&=\\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+\\gamma\\cdot R_{t+2}+\\gamma^2\\cdot v_\\pi(S_{t+2})\\mid S_t=s]\\\\\n",
    "&...\\\\\n",
    "&\\leq \\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+\\gamma\\cdot R_{t+2} + \\gamma ^2\\cdot R_{t+3}...\\mid S_t=s]\\\\\n",
    "&= \\quad v_{\\pi'}(s)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKzFw_Bp80eh"
   },
   "source": [
    "which holds true for all $s \\in S$, and note that $S_{t+1}$, $S_{t+2}$...are random variables. This demonstrates that the policy is always improving.\n",
    "\n",
    "## 2.2 Finite iteration\n",
    "\n",
    "A finite MDP has a finite number of states thus a finite number of policies. Policy iteration can go thourgh each one of them and it is inevitable that the algorithm finds one of the optimal policies in seom number of iterations. And once it finds that policy, the value function is optimal hence stops improving, making the policy stable therefore the algorithm terminates.\n",
    "\n",
    "## 2.3 Optimal Policy Guarantees\n",
    "\n",
    "If $v_{\\pi'}(s)=v_\\pi(s)$ for all $s \\in S$, then: $\\pi'(s)=\\max\\limits_aq_\\pi(s,a) = \\max\\limits_a\\sum\\limits_{s',r}p(s',r\\mid s,a)[r+\\gamma\\cdot v_{\\pi}(s')]$, for all $s \\in S$, which is exactly the Bellman optimality equation, therefore $v_{\\pi'}$ must be the optimal policy $v_*$; so does $v_\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KY5u0G2S82vd"
   },
   "source": [
    "## 3 Policy Iteration, Value Iteration and Improved Iteration algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "h-IGd_Ow3exw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP:\n",
    "    pass\n",
    "\n",
    "class SimpleMDP(MDP):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = ['s0', 's1']\n",
    "        self.actions = ['a0', 'a1', 'a2']\n",
    "        self.valid_actions = [[0,1],[2]]\n",
    "        # |S| * |A| -> |S|\n",
    "        self.transition = np.array(([\n",
    "            [\n",
    "                [0.5, 0.5],  # a0\n",
    "                [0, 1],  # a1\n",
    "                [0, 0]  # a2\n",
    "            ],  # state 0\n",
    "            [\n",
    "                [0, 0],\n",
    "                [0, 0],\n",
    "                [0, 1]\n",
    "            ]  # state 1\n",
    "        ]))\n",
    "        # |S| * |A| -> R\n",
    "        self.reward = np.array([\n",
    "            [5, 10, 0],\n",
    "            [0, 0, -1]\n",
    "        ])\n",
    "\n",
    "\n",
    "class PolicyLearner:\n",
    "\n",
    "    def __init__(self, mdp, discount=0.95, theta=1e-10):\n",
    "        self.mdp = mdp\n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "        self.nb_states = len(mdp.states)\n",
    "        self.nb_actions = len(mdp.actions)\n",
    "\n",
    "    def PolicyEvaluation(self, partial = False, partial_iteration_limit=100):\n",
    "        mask = [[False, False, True], [True, True, False]]\n",
    "        r_vec = np.sum(np.multiply(np.ma.masked_array(self.policy, mask),\n",
    "                                   np.ma.masked_array(self.mdp.reward, mask)), axis=1).data\n",
    "        # print(r_vec.shape)\n",
    "\n",
    "        p_mat = []\n",
    "        for i in range(self.nb_states):\n",
    "            trans = np.matmul(np.ma.masked_array(self.policy[i], mask[i]),\n",
    "                              np.ma.masked_array(self.mdp.transition[i], np.stack((mask[i], mask[i])).T)).data\n",
    "            p_mat.append(trans)\n",
    "        p_mat = np.array(p_mat)\n",
    "\n",
    "        count = 0\n",
    "        while (True):\n",
    "            count += 1\n",
    "\n",
    "            v = self.value_fun.copy()\n",
    "            self.value_fun = r_vec + self.discount * np.matmul(p_mat, self.value_fun)\n",
    "            delta = np.max(np.abs(v - self.value_fun))\n",
    "            # print(self.value_fun)\n",
    "            if delta < self.theta:\n",
    "                print('Sweep count:',count)\n",
    "                print('value function:', self.value_fun)\n",
    "                break\n",
    "\n",
    "            if partial is True and count >= partial_iteration_limit:\n",
    "                print('Sweep count:', count)\n",
    "                print('value function:', self.value_fun)\n",
    "                break\n",
    "\n",
    "\n",
    "    def PolicyImprovement(self):\n",
    "        stable = True\n",
    "        for i in range(self.nb_states):\n",
    "            # find the argmax action\n",
    "            q = np.full((self.nb_actions), fill_value=-np.inf)\n",
    "            for j in self.mdp.valid_actions[i]:\n",
    "                q[j] = self.mdp.reward[i, j] + self.discount * \\\n",
    "                        np.sum(np.multiply(self.mdp.transition[i, j], self.value_fun))\n",
    "            # print(q)\n",
    "            argmax_q = np.argmax(q)\n",
    "            # print(argmax_q)\n",
    "            prev_action = np.argmax(self.policy[i])\n",
    "            # print(q)\n",
    "            # print('state: {}, prev_q:{}, max_q:{}')\n",
    "            if prev_action != argmax_q:\n",
    "                print('policy update: (state {} -> action {})'.format(i, argmax_q))\n",
    "                stable = False\n",
    "                self.policy[i][prev_action] = 0\n",
    "                self.policy[i][argmax_q] = 1\n",
    "        return stable\n",
    "\n",
    "    def PolicyIteration(self):\n",
    "        # initialization\n",
    "        self.policy = np.zeros((self.nb_states, self.nb_actions))\n",
    "        #     for i in range(self.nb_states):\n",
    "        #       self.policy[i,int(np.random.rand()*3)] = 1\n",
    "        # only legit actions\n",
    "        self.policy[0, int(np.random.rand() * 2)] = 1\n",
    "        self.policy[1, 2] = 1\n",
    "        self.value_fun = np.zeros((self.nb_states,))\n",
    "\n",
    "        print('Initial policy:\\n', self.policy)\n",
    "\n",
    "        while (True):\n",
    "            self.PolicyEvaluation()\n",
    "            if self.PolicyImprovement():\n",
    "                print('Itertion finished')\n",
    "                print('policy:\\n', self.policy)\n",
    "                break\n",
    "\n",
    "    def ValueIteration(self):\n",
    "        # initialization\n",
    "        self.value_fun = np.zeros((self.nb_states,))\n",
    "        count = 0\n",
    "        while(True):\n",
    "            count += 1\n",
    "            v = self.value_fun.copy()\n",
    "            q = np.full((self.nb_states, self.nb_actions), fill_value=-np.inf)\n",
    "            for i in range(self.nb_states):\n",
    "                for j in self.mdp.valid_actions[i]:\n",
    "                    q[i][j] = self.mdp.reward[i, j] + self.discount * \\\n",
    "                           np.sum(np.multiply(self.mdp.transition[i, j], self.value_fun))\n",
    "                self.value_fun[i] = np.max(q[i])\n",
    "\n",
    "            # print(v)\n",
    "            # print(self.value_fun)\n",
    "            delta = np.max(np.abs(v - self.value_fun))\n",
    "\n",
    "            if delta < self.theta:\n",
    "                print('Sweep count:', count)\n",
    "                print('value function:', self.value_fun)\n",
    "                self.q = q\n",
    "                break\n",
    "        tmp = np.argmax(q, axis = 1)\n",
    "        self.policy = np.zeros((self.nb_states, self.nb_actions))\n",
    "        for i in range(self.nb_states):\n",
    "            self.policy[i][tmp[i]] = 1\n",
    "        print('Policy:\\n', self.policy)\n",
    "\n",
    "    def ImprovedPolicyIteration(self, partial_iteration_limit):\n",
    "        self.policy = np.zeros((self.nb_states, self.nb_actions))\n",
    "        self.policy[0, int(np.random.rand() * 2)] = 1\n",
    "        self.policy[1, 2] = 1\n",
    "        self.value_fun = np.zeros((self.nb_states,))\n",
    "        print('Initial policy:\\n', self.policy)\n",
    "        while (True):\n",
    "            self.PolicyEvaluation(partial=True, partial_iteration_limit=partial_iteration_limit)\n",
    "            if self.PolicyImprovement():\n",
    "                print('Itertion finished')\n",
    "                print('policy:\\n', self.policy)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5t0uOPG_hsJz"
   },
   "source": [
    "Test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 578,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1517608691761,
     "user": {
      "displayName": "Zichao Yan",
      "photoUrl": "//lh3.googleusercontent.com/-lUGgNeiX7v8/AAAAAAAAAAI/AAAAAAAAABE/6cWBoWlDDyc/s50-c-k-no/photo.jpg",
      "userId": "105794893504586740188"
     },
     "user_tz": 300
    },
    "id": "prIBCzxUgghW",
    "outputId": "96ac7775-e0fd-4d6c-afd3-81dd2945f5db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration for exmaple MDP\n",
      "Initial policy:\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "Sweep count: 450\n",
      "value function: [ -8.57142857 -20.        ]\n",
      "Itertion finished\n",
      "policy:\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "\n",
      "Value Iteration for example MDP\n",
      "Sweep count: 450\n",
      "value function: [ -8.57142857 -20.        ]\n",
      "Policy:\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "\n",
      "Partial Policy Iteration for example MDP\n",
      "Initial policy:\n",
      " [[0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "Sweep count: 50\n",
      "value function: [ -7.46110049 -18.46110049]\n",
      "policy update: (state 0 -> action 0)\n",
      "Sweep count: 50\n",
      "value function: [ -8.45301799 -19.88158942]\n",
      "Itertion finished\n",
      "policy:\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "learner = PolicyLearner(SimpleMDP())\n",
    "print('Policy Iteration for exmaple MDP')\n",
    "learner.PolicyIteration()\n",
    "print('\\n')\n",
    "print('Value Iteration for example MDP')\n",
    "learner.ValueIteration()\n",
    "print('\\n')\n",
    "print('Partial Policy Iteration for example MDP')\n",
    "learner.ImprovedPolicyIteration(partial_iteration_limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bKGCMX3h7WO"
   },
   "source": [
    "A second MDP: Small Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "e-5AqyhehrRp"
   },
   "outputs": [],
   "source": [
    "class GridMDP(MDP):\n",
    "    '''\n",
    "        A 4 by 4 grid.\n",
    "        Equivalent to the Small Gridworld in course slides.\n",
    "        '''\n",
    "    def __init__(self):\n",
    "        # only non terminal states\n",
    "        self.states = np.arange(16)\n",
    "        self.nonterminal_states = np.arange(1,15)\n",
    "        self.actions = ['top', 'right', 'down', 'left']\n",
    "\n",
    "        self.transition = []\n",
    "        self.reward = []\n",
    "        for s in self.states:\n",
    "            mat = np.zeros((len(self.actions), len(self.states)))\n",
    "            if s-4>=0:\n",
    "                mat[0][s - 4] = 1\n",
    "            else:\n",
    "                mat[0][s] = 1\n",
    "            if (s+1)%4!=0:\n",
    "                mat[1][s + 1] = 1\n",
    "            else:\n",
    "                mat[1][s] = 1\n",
    "            if s+4<=15:\n",
    "                mat[2][s + 4] = 1\n",
    "            else:\n",
    "                mat[2][s] = 1\n",
    "            if s%4!=0:\n",
    "                mat[3][s - 1] = 1\n",
    "            else:\n",
    "                mat[3][s] = 1\n",
    "            self.transition.append(mat)\n",
    "            self.reward.append(np.full(len(self.actions), fill_value=-1))\n",
    "        self.transition = np.array(self.transition)\n",
    "        self.reward = np.array(self.reward)\n",
    "        # self.reward[0] = np.zeros(4)\n",
    "        # self.reward[15] = np.zeros(4)\n",
    "\n",
    "\n",
    "class GridPolicyLearner:\n",
    "\n",
    "    def __init__(self, mdp, discount=0.95, theta=1e-10):\n",
    "        self.mdp = mdp\n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "        self.nb_states = len(mdp.states)\n",
    "        self.nb_actions = len(mdp.actions)\n",
    "\n",
    "    def print_value_fun(self):\n",
    "        for i in (0, 4, 8, 12):\n",
    "            print(\"\\t\".join(['{:.2f}'.format(c) for c in self.value_fun[i:i + 4]]))\n",
    "\n",
    "    def print_actions(self):\n",
    "        vis = [['terminal', 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 'terminal']]\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i, j) == (0, 0) or (i, j) == (3, 3):\n",
    "                    continue\n",
    "                vis[i][j] = self.mdp.actions[np.argmax(self.policy[4 * i + j])]\n",
    "        print('policy:')\n",
    "        # print(self.policy)\n",
    "        for row in vis:\n",
    "            print(\"{:<12} {:<12} {:<12} {:<12}\".format(row[0], row[1], row[2], row[3]))\n",
    "\n",
    "    def PolicyEvaluation(self, partial = False, partial_iteration_limit=100):\n",
    "        r_vec = np.sum(np.multiply(self.policy, self.mdp.reward), axis=1)\n",
    "        # print(r_vec.shape)\n",
    "\n",
    "        p_mat = []\n",
    "        for i in range(self.nb_states):\n",
    "            trans = np.matmul(self.policy[i], self.mdp.transition[i])\n",
    "            p_mat.append(trans)\n",
    "        p_mat = np.array(p_mat)\n",
    "\n",
    "        count = 0\n",
    "        while (True):\n",
    "            count += 1\n",
    "\n",
    "            v = self.value_fun.copy()\n",
    "            self.value_fun = r_vec + self.discount * np.matmul(p_mat, self.value_fun)\n",
    "            delta = np.max(np.abs(v - self.value_fun))\n",
    "            # print(self.value_fun)\n",
    "            if delta < self.theta:\n",
    "                print('Sweep count:',count)\n",
    "                print('value function:')\n",
    "                self.print_value_fun()\n",
    "                break\n",
    "\n",
    "            if partial is True and count >= partial_iteration_limit:\n",
    "                print('Sweep count:', count)\n",
    "                print('value function:')\n",
    "                self.print_value_fun()\n",
    "                break\n",
    "\n",
    "\n",
    "    def PolicyImprovement(self):\n",
    "        stable = True\n",
    "        for i in range(self.nb_states):\n",
    "            # don't need to update actions for terminal states â€” they have no actions.\n",
    "            if i == 0 or i == 15:\n",
    "                continue\n",
    "            # find the argmax action\n",
    "            q = np.full((self.nb_actions), fill_value=-np.inf)\n",
    "            for j in range(self.nb_actions):\n",
    "                q[j] = self.mdp.reward[i, j] + self.discount * \\\n",
    "                        np.sum(np.multiply(self.mdp.transition[i, j], self.value_fun))\n",
    "\n",
    "            argmax_q = np.argmax(q)\n",
    "            prev_action = np.argmax(self.policy[i])\n",
    "\n",
    "            if prev_action != argmax_q:\n",
    "                print('policy update: (state {} -> action {})'.format(i, argmax_q))\n",
    "                stable = False\n",
    "                self.policy[i] = np.zeros(self.nb_actions)\n",
    "                self.policy[i][argmax_q] = 1\n",
    "            else:\n",
    "                self.policy[i] = np.zeros(self.nb_actions)\n",
    "                self.policy[i][prev_action] = 1\n",
    "        return stable\n",
    "\n",
    "    def PolicyIteration(self):\n",
    "        # initialization\n",
    "        self.policy = np.full((self.nb_states, self.nb_actions), fill_value=0.25)\n",
    "        # mask out actions for state 0 and state 1\n",
    "        self.policy[0,:] = np.zeros(self.nb_actions)\n",
    "        self.policy[15, :] = np.zeros(self.nb_actions)\n",
    "        self.value_fun = np.zeros((self.nb_states,))\n",
    "\n",
    "#        print('Initial policy:\\n', self.policy)\n",
    "\n",
    "        while (True):\n",
    "            self.PolicyEvaluation()\n",
    "            if self.PolicyImprovement():\n",
    "                print('Itertion finished')\n",
    "                self.print_actions()\n",
    "                break\n",
    "\n",
    "    def ValueIteration(self):\n",
    "        # initialization\n",
    "        self.value_fun = np.zeros((self.nb_states,))\n",
    "        count = 0\n",
    "        while(True):\n",
    "            count += 1\n",
    "            v = self.value_fun.copy()\n",
    "            q = np.full((self.nb_states, self.nb_actions), fill_value=-np.inf)\n",
    "            for i in range(self.nb_states):\n",
    "                if i==0 or i==15:\n",
    "                    continue\n",
    "                for j in range(self.nb_actions):\n",
    "                    q[i][j] = self.mdp.reward[i, j] + self.discount * \\\n",
    "                           np.sum(np.multiply(self.mdp.transition[i, j], self.value_fun))\n",
    "                self.value_fun[i] = np.max(q[i])\n",
    "\n",
    "            delta = np.max(np.abs(v - self.value_fun))\n",
    "\n",
    "            if delta < self.theta:\n",
    "                print('Sweep count:', count)\n",
    "                print('value function:')\n",
    "                self.print_value_fun()\n",
    "                self.q = q\n",
    "                break\n",
    "        tmp = np.argmax(q, axis = 1)\n",
    "        self.policy = np.zeros((self.nb_states, self.nb_actions))\n",
    "        for i in range(self.nb_states):\n",
    "            self.policy[i][tmp[i]] = 1\n",
    "        self.print_actions()\n",
    "\n",
    "    def ImprovedPolicyIteration(self, partial_iteration_limit):\n",
    "        # initialization\n",
    "        self.policy = np.full((self.nb_states, self.nb_actions), fill_value=0.25)\n",
    "        # mask out actions for state 0 and state 1\n",
    "        self.policy[0, :] = np.zeros(self.nb_actions)\n",
    "        self.policy[15, :] = np.zeros(self.nb_actions)\n",
    "        self.value_fun = np.zeros((self.nb_states,))\n",
    "\n",
    "        while (True):\n",
    "            self.PolicyEvaluation(partial=True, partial_iteration_limit=partial_iteration_limit)\n",
    "            if self.PolicyImprovement():\n",
    "                print('Itertion finished')\n",
    "                self.print_actions()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1517615462549,
     "user": {
      "displayName": "Zichao Yan",
      "photoUrl": "//lh3.googleusercontent.com/-lUGgNeiX7v8/AAAAAAAAAAI/AAAAAAAAABE/6cWBoWlDDyc/s50-c-k-no/photo.jpg",
      "userId": "105794893504586740188"
     },
     "user_tz": 300
    },
    "id": "u5mLPquJ7i_m",
    "outputId": "241f7af8-70b2-4b8b-a2a9-55ac32dcfc68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration for Gridworld\n",
      "Sweep count: 221\n",
      "value function:\n",
      "0.00\t-7.59\t-10.53\t-11.43\n",
      "-7.59\t-9.63\t-10.58\t-10.53\n",
      "-10.53\t-10.58\t-9.63\t-7.59\n",
      "-11.43\t-10.53\t-7.59\t0.00\n",
      "policy update: (state 1 -> action 3)\n",
      "policy update: (state 2 -> action 3)\n",
      "policy update: (state 3 -> action 2)\n",
      "policy update: (state 6 -> action 2)\n",
      "policy update: (state 7 -> action 2)\n",
      "policy update: (state 10 -> action 1)\n",
      "policy update: (state 11 -> action 2)\n",
      "policy update: (state 13 -> action 1)\n",
      "policy update: (state 14 -> action 1)\n",
      "Sweep count: 4\n",
      "value function:\n",
      "0.00\t-1.00\t-1.95\t-2.85\n",
      "-1.00\t-1.95\t-2.85\t-1.95\n",
      "-1.95\t-2.85\t-1.95\t-1.00\n",
      "-2.85\t-1.95\t-1.00\t0.00\n",
      "policy update: (state 6 -> action 0)\n",
      "Sweep count: 1\n",
      "value function:\n",
      "0.00\t-1.00\t-1.95\t-2.85\n",
      "-1.00\t-1.95\t-2.85\t-1.95\n",
      "-1.95\t-2.85\t-1.95\t-1.00\n",
      "-2.85\t-1.95\t-1.00\t0.00\n",
      "Itertion finished\n",
      "policy:\n",
      "terminal     left         left         down        \n",
      "top          top          top          down        \n",
      "top          top          right        down        \n",
      "top          right        right        terminal    \n",
      "\n",
      "\n",
      "Value Iteration for Gridworld\n",
      "Sweep count: 4\n",
      "value function:\n",
      "0.00\t-1.00\t-1.95\t-2.85\n",
      "-1.00\t-1.95\t-2.85\t-1.95\n",
      "-1.95\t-2.85\t-1.95\t-1.00\n",
      "-2.85\t-1.95\t-1.00\t0.00\n",
      "policy:\n",
      "terminal     left         left         down        \n",
      "top          top          top          down        \n",
      "top          top          right        down        \n",
      "top          right        right        terminal    \n",
      "\n",
      "\n",
      "Modified Value Iteration for Gridworld\n",
      "Sweep count: 50\n",
      "value function:\n",
      "0.00\t-7.56\t-10.48\t-11.37\n",
      "-7.56\t-9.58\t-10.52\t-10.48\n",
      "-10.48\t-10.52\t-9.58\t-7.56\n",
      "-11.37\t-10.48\t-7.56\t0.00\n",
      "policy update: (state 1 -> action 3)\n",
      "policy update: (state 2 -> action 3)\n",
      "policy update: (state 3 -> action 2)\n",
      "policy update: (state 6 -> action 2)\n",
      "policy update: (state 7 -> action 2)\n",
      "policy update: (state 10 -> action 1)\n",
      "policy update: (state 11 -> action 2)\n",
      "policy update: (state 12 -> action 1)\n",
      "policy update: (state 13 -> action 1)\n",
      "policy update: (state 14 -> action 1)\n",
      "Sweep count: 4\n",
      "value function:\n",
      "0.00\t-1.00\t-1.95\t-2.85\n",
      "-1.00\t-1.95\t-2.85\t-1.95\n",
      "-1.95\t-2.85\t-1.95\t-1.00\n",
      "-2.85\t-1.95\t-1.00\t0.00\n",
      "policy update: (state 6 -> action 0)\n",
      "policy update: (state 12 -> action 0)\n",
      "Sweep count: 1\n",
      "value function:\n",
      "0.00\t-1.00\t-1.95\t-2.85\n",
      "-1.00\t-1.95\t-2.85\t-1.95\n",
      "-1.95\t-2.85\t-1.95\t-1.00\n",
      "-2.85\t-1.95\t-1.00\t0.00\n",
      "Itertion finished\n",
      "policy:\n",
      "terminal     left         left         down        \n",
      "top          top          top          down        \n",
      "top          top          right        down        \n",
      "top          right        right        terminal    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learner = GridPolicyLearner(GridMDP())\n",
    "print('Policy Iteration for Gridworld')\n",
    "learner.PolicyIteration()\n",
    "print('\\n')\n",
    "print('Value Iteration for Gridworld')\n",
    "learner.ValueIteration()\n",
    "print('\\n')\n",
    "print('Modified Value Iteration for Gridworld')\n",
    "learner.ImprovedPolicyIteration(partial_iteration_limit=50)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "i4VqbWOb7jcZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "COMP767 Assignment 2 - MDP (Track 1)",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
