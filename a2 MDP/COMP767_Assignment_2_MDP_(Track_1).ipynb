{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP767 Assignment 2 - MDP (Track 1)",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "hhHyCOZglf7m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Bellman Optimality Equations\n",
        "$$\n",
        "\\begin{align}\n",
        "\\epsilon_{k+1}&= \\quad || v_{k+1}(s)-v_{*}(s)||_{\\infty}\\quad\\\\\n",
        "&= \\quad \\max\\limits_s(\\vert\\max\\limits_a(\\sum\\limits_{s',r}P(s',r|s,a)[r+\\gamma\\cdot v_{k}(s')] - \\max\\limits_a'(\\sum\\limits_{s',r}P(s',r|s,a')[r+\\gamma\\cdot v_{*}(s')]\\vert)\\\\\n",
        "&= \\quad \\max\\limits_s(\\vert\\max\\limits_{a}(\\sum\\limits_{s',r}P(s',r|s,a) \\cdot \\gamma \\cdot[v_k(s')-v_*(s')])\\vert)\\\\\n",
        "&\\leq \\quad \\gamma \\cdot\\vert \\max_{s'}[v_k(s')-v_*(s')] \\vert\\\\\n",
        "&= \\quad\\gamma\\cdot||v_k(s)-v_*(s)||_{\\infty}\\\\\n",
        "&= \\quad \\gamma \\cdot \\epsilon_{k} \\quad \\gamma \\in (0,1), k=1...T\\\\\n",
        "&s.t. \\quad \\epsilon_{k+1} \\leq \\gamma^k \\epsilon_1\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "zooOwVfJ8vQ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Therefore Bellman optimality equation converges."
      ]
    },
    {
      "metadata": {
        "id": "vaN7VcXxpVGo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Policy Iteration\n",
        "## 2.1 Policy Improvement Guarantees\n",
        "Let's say we have an old policy $\\pi$ and a changed policy $\\pi'$. \n",
        "\n",
        "In policy improvement phase, $\\pi'(s)=\\max\\limits_aq_\\pi(s,a)$, for $s \\in S$.\n",
        "\n",
        "So, $v_\\pi(s) \\leq q_\\pi(s,\\pi'(s))$, that is to say, the old policy can be improved by choosing a new immediate action denoted as $\\pi'(s)$ and then follows itself again.\n",
        "\n",
        "Therefore it follows that:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "v_\\pi(s) \\quad &\\leq \\quad q_\\pi(s,\\pi'(s))\\\\\n",
        "&= \\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+\\gamma\\cdot v_\\pi(S_{t+1})\\mid S_t=s]\\\\\n",
        "&\\leq \\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+ \\gamma\\cdot q_\\pi(S_{t+1},\\pi'(S_{t+1}))\\mid S_t=s]\\\\\n",
        "&= \\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+\\gamma\\cdot\\mathop{\\mathbb{E}_{\\pi'}}[R_{t+2}+\\gamma\\cdot v_\\pi(S_{t+2})\\mid S_t=s]\\\\\n",
        "&=\\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+\\gamma\\cdot R_{t+2}+\\gamma^2\\cdot v_\\pi(S_{t+2})\\mid S_t=s]\\\\\n",
        "&...\\\\\n",
        "&\\leq \\quad \\mathop{\\mathbb{E}_{\\pi'}}[R_{t+1}+\\gamma\\cdot R_{t+2} + \\gamma ^2\\cdot R_{t+3}...\\mid S_t=s]\\\\\n",
        "&= \\quad v_{\\pi'}(s)\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "EKzFw_Bp80eh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "which holds true for all $s \\in S$, and note that $S_{t+1}$, $S_{t+2}$...are random variables. This demonstrates that the policy is always improving.\n",
        "\n",
        "## 2.2 Finite iteration\n",
        "\n",
        "A finite MDP has a finite number of states thus a finite number of policies. Policy iteration can go thourgh each one of them and it is inevitable that the algorithm finds one of the optimal policies in seom number of iterations. And once it finds that policy, the value function is optimal hence stops improving, making the policy stable therefore the algorithm terminates.\n",
        "\n",
        "## 2.3 Optimal Policy Guarantees\n",
        "\n",
        "If $v_{\\pi'}(s)=v_\\pi(s)$ for all $s \\in S$, then: $\\pi'(s)=\\max\\limits_aq_\\pi(s,a) = \\max\\limits_a\\sum\\limits_{s',r}p(s',r\\mid s,a)[r+\\gamma\\cdot v_{\\pi}(s')]$, for all $s \\in S$, which is exactly the Bellman optimality equation, therefore $v_{\\pi'}$ must be the optimal policy $v_*$; so does $v_\\pi$."
      ]
    },
    {
      "metadata": {
        "id": "KY5u0G2S82vd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 Policy Iteration, Value Iteration and Improved Iteration algorithms\n",
        "### 3.1 Simple MDP"
      ]
    },
    {
      "metadata": {
        "id": "h-IGd_Ow3exw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDP:\n",
        "    pass\n",
        "\n",
        "class SimpleMDP(MDP):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = ['s0', 's1']\n",
        "        self.actions = ['a0', 'a1', 'a2']\n",
        "        self.valid_actions = [[0,1],[2]]\n",
        "        # |S| * |A| -> |S|\n",
        "        self.transition = np.array(([\n",
        "            [\n",
        "                [0.5, 0.5],  # a0\n",
        "                [0, 1],  # a1\n",
        "                [0, 0]  # a2\n",
        "            ],  # state 0\n",
        "            [\n",
        "                [0, 0],\n",
        "                [0, 0],\n",
        "                [0, 1]\n",
        "            ]  # state 1\n",
        "        ]))\n",
        "        # |S| * |A| -> R\n",
        "        self.reward = np.array([\n",
        "            [5, 10, 0],\n",
        "            [0, 0, -1]\n",
        "        ])\n",
        "\n",
        "\n",
        "class PolicyLearner:\n",
        "\n",
        "    def __init__(self, mdp, discount=0.95, theta=1e-10):\n",
        "        self.mdp = mdp\n",
        "        self.discount = discount\n",
        "        self.theta = theta\n",
        "        self.nb_states = len(mdp.states)\n",
        "        self.nb_actions = len(mdp.actions)\n",
        "\n",
        "    def PolicyEvaluation(self, partial = False, partial_iteration_limit=100):\n",
        "        mask = [[False, False, True], [True, True, False]]\n",
        "        r_vec = np.sum(np.multiply(np.ma.masked_array(self.policy, mask),\n",
        "                                   np.ma.masked_array(self.mdp.reward, mask)), axis=1).data\n",
        "        # print(r_vec.shape)\n",
        "\n",
        "        p_mat = []\n",
        "        for i in range(self.nb_states):\n",
        "            trans = np.matmul(np.ma.masked_array(self.policy[i], mask[i]),\n",
        "                              np.ma.masked_array(self.mdp.transition[i], np.stack((mask[i], mask[i])).T)).data\n",
        "            p_mat.append(trans)\n",
        "        p_mat = np.array(p_mat)\n",
        "\n",
        "        count = 0\n",
        "        while (True):\n",
        "            count += 1\n",
        "\n",
        "            v = self.value_fun.copy()\n",
        "            self.value_fun = r_vec + self.discount * np.matmul(p_mat, self.value_fun)\n",
        "            delta = np.max(np.abs(v - self.value_fun))\n",
        "            # print(self.value_fun)\n",
        "            if delta < self.theta:\n",
        "                print('Sweep count:',count)\n",
        "                print('value function:', self.value_fun)\n",
        "                break\n",
        "\n",
        "            if partial is True and count >= partial_iteration_limit:\n",
        "                print('Sweep count:', count)\n",
        "                print('value function:', self.value_fun)\n",
        "                break\n",
        "\n",
        "\n",
        "    def PolicyImprovement(self):\n",
        "        stable = True\n",
        "        for i in range(self.nb_states):\n",
        "            # find the argmax action\n",
        "            q = np.full((self.nb_actions), fill_value=-np.inf)\n",
        "            for j in self.mdp.valid_actions[i]:\n",
        "                q[j] = self.mdp.reward[i, j] + self.discount * \\\n",
        "                        np.sum(np.multiply(self.mdp.transition[i, j], self.value_fun))\n",
        "            # print(q)\n",
        "            argmax_q = np.argmax(q)\n",
        "            # print(argmax_q)\n",
        "            prev_action = np.argmax(self.policy[i])\n",
        "            # print(q)\n",
        "            # print('state: {}, prev_q:{}, max_q:{}')\n",
        "            if prev_action != argmax_q:\n",
        "                print('policy update: (state {} -> action {})'.format(i, argmax_q))\n",
        "                stable = False\n",
        "                self.policy[i][prev_action] = 0\n",
        "                self.policy[i][argmax_q] = 1\n",
        "        return stable\n",
        "\n",
        "    def PolicyIteration(self):\n",
        "        # initialization\n",
        "        self.policy = np.zeros((self.nb_states, self.nb_actions))\n",
        "        #     for i in range(self.nb_states):\n",
        "        #       self.policy[i,int(np.random.rand()*3)] = 1\n",
        "        # only legit actions\n",
        "        self.policy[0, int(np.random.rand() * 2)] = 1\n",
        "        self.policy[1, 2] = 1\n",
        "        self.value_fun = np.zeros((self.nb_states,))\n",
        "\n",
        "        print('Initial policy:\\n', self.policy)\n",
        "\n",
        "        while (True):\n",
        "            self.PolicyEvaluation()\n",
        "            if self.PolicyImprovement():\n",
        "                print('Itertion finished')\n",
        "                print('policy:\\n', self.policy)\n",
        "                break\n",
        "\n",
        "    def ValueIteration(self):\n",
        "        # initialization\n",
        "        self.value_fun = np.zeros((self.nb_states,))\n",
        "        count = 0\n",
        "        while(True):\n",
        "            count += 1\n",
        "            v = self.value_fun.copy()\n",
        "            q = np.full((self.nb_states, self.nb_actions), fill_value=-np.inf)\n",
        "            for i in range(self.nb_states):\n",
        "                for j in self.mdp.valid_actions[i]:\n",
        "                    q[i][j] = self.mdp.reward[i, j] + self.discount * \\\n",
        "                           np.sum(np.multiply(self.mdp.transition[i, j], self.value_fun))\n",
        "                self.value_fun[i] = np.max(q[i])\n",
        "\n",
        "            # print(v)\n",
        "            # print(self.value_fun)\n",
        "            delta = np.max(np.abs(v - self.value_fun))\n",
        "\n",
        "            if delta < self.theta:\n",
        "                print('Sweep count:', count)\n",
        "                print('value function:', self.value_fun)\n",
        "                self.q = q\n",
        "                break\n",
        "        tmp = np.argmax(q, axis = 1)\n",
        "        self.policy = np.zeros((self.nb_states, self.nb_actions))\n",
        "        for i in range(self.nb_states):\n",
        "            self.policy[i][tmp[i]] = 1\n",
        "        print('Policy:\\n', self.policy)\n",
        "\n",
        "    def ImprovedPolicyIteration(self, partial_iteration_limit):\n",
        "        self.policy = np.zeros((self.nb_states, self.nb_actions))\n",
        "        self.policy[0, int(np.random.rand() * 2)] = 1\n",
        "        self.policy[1, 2] = 1\n",
        "        self.value_fun = np.zeros((self.nb_states,))\n",
        "        print('Initial policy:\\n', self.policy)\n",
        "        while (True):\n",
        "            self.PolicyEvaluation(partial=True, partial_iteration_limit=partial_iteration_limit)\n",
        "            if self.PolicyImprovement():\n",
        "                print('Itertion finished')\n",
        "                print('policy:\\n', self.policy)\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5t0uOPG_hsJz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test run."
      ]
    },
    {
      "metadata": {
        "id": "prIBCzxUgghW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "e111f49a-db45-4ce8-9bdb-cc813e565045",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517697728327,
          "user_tz": 300,
          "elapsed": 423,
          "user": {
            "displayName": "Zichao Yan",
            "photoUrl": "//lh3.googleusercontent.com/-lUGgNeiX7v8/AAAAAAAAAAI/AAAAAAAAABE/6cWBoWlDDyc/s50-c-k-no/photo.jpg",
            "userId": "105794893504586740188"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "learner = PolicyLearner(SimpleMDP())\n",
        "print('Policy Iteration for exmaple MDP')\n",
        "learner.PolicyIteration()\n",
        "print('\\n')\n",
        "print('Value Iteration for example MDP')\n",
        "learner.ValueIteration()\n",
        "print('\\n')\n",
        "print('Partial Policy Iteration for example MDP')\n",
        "learner.ImprovedPolicyIteration(partial_iteration_limit=50)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy Iteration for exmaple MDP\n",
            "Initial policy:\n",
            " [[1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Sweep count: 450\n",
            "value function: [ -8.57142857 -20.        ]\n",
            "Itertion finished\n",
            "policy:\n",
            " [[1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "\n",
            "\n",
            "Value Iteration for example MDP\n",
            "Sweep count: 450\n",
            "value function: [ -8.57142857 -20.        ]\n",
            "Policy:\n",
            " [[1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "\n",
            "\n",
            "Partial Policy Iteration for example MDP\n",
            "Initial policy:\n",
            " [[0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "Sweep count: 50\n",
            "value function: [ -7.46110049 -18.46110049]\n",
            "policy update: (state 0 -> action 0)\n",
            "Sweep count: 50\n",
            "value function: [ -8.45301799 -19.88158942]\n",
            "Itertion finished\n",
            "policy:\n",
            " [[1. 0. 0.]\n",
            " [0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_bKGCMX3h7WO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 Gridworld MDP\n",
        "A 4 $\\times$ 4 Grid MDP, equivalent to the one on the slide."
      ]
    },
    {
      "metadata": {
        "id": "e-5AqyhehrRp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class GridMDP(MDP):\n",
        "    '''\n",
        "        A 4 by 4 grid.\n",
        "        Equivalent to the Small Gridworld in course slides.\n",
        "        '''\n",
        "    def __init__(self):\n",
        "        # only non terminal states\n",
        "        self.states = np.arange(16)\n",
        "        self.nonterminal_states = np.arange(1,15)\n",
        "        self.actions = ['top', 'right', 'down', 'left']\n",
        "\n",
        "        self.transition = []\n",
        "        self.reward = []\n",
        "        for s in self.states:\n",
        "            mat = np.zeros((len(self.actions), len(self.states)))\n",
        "            if s-4>=0:\n",
        "                mat[0][s - 4] = 1\n",
        "            else:\n",
        "                mat[0][s] = 1\n",
        "            if (s+1)%4!=0:\n",
        "                mat[1][s + 1] = 1\n",
        "            else:\n",
        "                mat[1][s] = 1\n",
        "            if s+4<=15:\n",
        "                mat[2][s + 4] = 1\n",
        "            else:\n",
        "                mat[2][s] = 1\n",
        "            if s%4!=0:\n",
        "                mat[3][s - 1] = 1\n",
        "            else:\n",
        "                mat[3][s] = 1\n",
        "            self.transition.append(mat)\n",
        "            self.reward.append(np.full(len(self.actions), fill_value=-1))\n",
        "        self.transition = np.array(self.transition)\n",
        "        self.reward = np.array(self.reward)\n",
        "        # self.reward[0] = np.zeros(4)\n",
        "        # self.reward[15] = np.zeros(4)\n",
        "\n",
        "\n",
        "class GridPolicyLearner:\n",
        "\n",
        "    def __init__(self, mdp, discount=0.95, theta=1e-10):\n",
        "        self.mdp = mdp\n",
        "        self.discount = discount\n",
        "        self.theta = theta\n",
        "        self.nb_states = len(mdp.states)\n",
        "        self.nb_actions = len(mdp.actions)\n",
        "\n",
        "    def print_value_fun(self):\n",
        "        for i in (0, 4, 8, 12):\n",
        "            print(\"\\t\".join(['{:.2f}'.format(c) for c in self.value_fun[i:i + 4]]))\n",
        "\n",
        "    def print_actions(self):\n",
        "        vis = [['terminal', 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 'terminal']]\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                if (i, j) == (0, 0) or (i, j) == (3, 3):\n",
        "                    continue\n",
        "                vis[i][j] = self.mdp.actions[np.argmax(self.policy[4 * i + j])]\n",
        "        print('policy:')\n",
        "        # print(self.policy)\n",
        "        for row in vis:\n",
        "            print(\"{:<12} {:<12} {:<12} {:<12}\".format(row[0], row[1], row[2], row[3]))\n",
        "\n",
        "    def PolicyEvaluation(self, partial = False, partial_iteration_limit=100):\n",
        "        r_vec = np.sum(np.multiply(self.policy, self.mdp.reward), axis=1)\n",
        "        # print(r_vec.shape)\n",
        "\n",
        "        p_mat = []\n",
        "        for i in range(self.nb_states):\n",
        "            trans = np.matmul(self.policy[i], self.mdp.transition[i])\n",
        "            p_mat.append(trans)\n",
        "        p_mat = np.array(p_mat)\n",
        "\n",
        "        count = 0\n",
        "        while (True):\n",
        "            count += 1\n",
        "\n",
        "            v = self.value_fun.copy()\n",
        "            self.value_fun = r_vec + self.discount * np.matmul(p_mat, self.value_fun)\n",
        "            delta = np.max(np.abs(v - self.value_fun))\n",
        "            # print(self.value_fun)\n",
        "            if delta < self.theta:\n",
        "                print('Sweep count:',count)\n",
        "                print('value function:')\n",
        "                self.print_value_fun()\n",
        "                break\n",
        "\n",
        "            if partial is True and count >= partial_iteration_limit:\n",
        "                print('Sweep count:', count)\n",
        "                print('value function:')\n",
        "                self.print_value_fun()\n",
        "                break\n",
        "        return count\n",
        "\n",
        "\n",
        "    def PolicyImprovement(self):\n",
        "        stable = True\n",
        "        for i in range(self.nb_states):\n",
        "            # don't need to update actions for terminal states â€” they have no actions.\n",
        "            if i == 0 or i == 15:\n",
        "                continue\n",
        "            # find the argmax action\n",
        "            q = np.full((self.nb_actions), fill_value=-np.inf)\n",
        "            for j in range(self.nb_actions):\n",
        "                q[j] = self.mdp.reward[i, j] + self.discount * \\\n",
        "                        np.sum(np.multiply(self.mdp.transition[i, j], self.value_fun))\n",
        "\n",
        "            argmax_q = np.argmax(q)\n",
        "            prev_action = np.argmax(self.policy[i])\n",
        "\n",
        "            if prev_action != argmax_q:\n",
        "                print('policy update: (state {} -> action {})'.format(i, argmax_q))\n",
        "                stable = False\n",
        "                self.policy[i] = np.zeros(self.nb_actions)\n",
        "                self.policy[i][argmax_q] = 1\n",
        "            else:\n",
        "                self.policy[i] = np.zeros(self.nb_actions)\n",
        "                self.policy[i][prev_action] = 1\n",
        "        return stable\n",
        "\n",
        "    def PolicyIteration(self):\n",
        "        # initialization\n",
        "        total = 0\n",
        "        self.policy = np.full((self.nb_states, self.nb_actions), fill_value=0.25)\n",
        "        # mask out actions for state 0 and state 1\n",
        "        self.policy[0,:] = np.zeros(self.nb_actions)\n",
        "        self.policy[15, :] = np.zeros(self.nb_actions)\n",
        "        self.value_fun = np.zeros((self.nb_states,))\n",
        "\n",
        "#        print('Initial policy:\\n', self.policy)\n",
        "\n",
        "        while (True):\n",
        "            total += self.PolicyEvaluation()\n",
        "            if self.PolicyImprovement():\n",
        "                print('Itertion finished')\n",
        "                self.print_actions()\n",
        "                break\n",
        "        return total\n",
        "\n",
        "    def ValueIteration(self):\n",
        "        # initialization\n",
        "        self.value_fun = np.zeros((self.nb_states,))\n",
        "        count = 0\n",
        "        while(True):\n",
        "            count += 1\n",
        "            v = self.value_fun.copy()\n",
        "            q = np.full((self.nb_states, self.nb_actions), fill_value=-np.inf)\n",
        "            for i in range(self.nb_states):\n",
        "                if i==0 or i==15:\n",
        "                    continue\n",
        "                for j in range(self.nb_actions):\n",
        "                    q[i][j] = self.mdp.reward[i, j] + self.discount * \\\n",
        "                           np.sum(np.multiply(self.mdp.transition[i, j], self.value_fun))\n",
        "                self.value_fun[i] = np.max(q[i])\n",
        "\n",
        "            delta = np.max(np.abs(v - self.value_fun))\n",
        "\n",
        "            if delta < self.theta:\n",
        "                print('Sweep count:', count)\n",
        "                print('value function:')\n",
        "                self.print_value_fun()\n",
        "                self.q = q\n",
        "                break\n",
        "        tmp = np.argmax(q, axis = 1)\n",
        "        self.policy = np.zeros((self.nb_states, self.nb_actions))\n",
        "        for i in range(self.nb_states):\n",
        "            self.policy[i][tmp[i]] = 1\n",
        "        self.print_actions()\n",
        "        return count\n",
        "\n",
        "    def ImprovedPolicyIteration(self, partial_iteration_limit):\n",
        "        # initialization\n",
        "        total = 0\n",
        "        self.policy = np.full((self.nb_states, self.nb_actions), fill_value=0.25)\n",
        "        # mask out actions for state 0 and state 1\n",
        "        self.policy[0, :] = np.zeros(self.nb_actions)\n",
        "        self.policy[15, :] = np.zeros(self.nb_actions)\n",
        "        self.value_fun = np.zeros((self.nb_states,))\n",
        "\n",
        "        while (True):\n",
        "            total += self.PolicyEvaluation(partial=True, partial_iteration_limit=partial_iteration_limit)\n",
        "            if self.PolicyImprovement():\n",
        "                print('Itertion finished')\n",
        "                self.print_actions()\n",
        "                break\n",
        "        return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u5mLPquJ7i_m",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1547
        },
        "outputId": "7d05e566-c44b-485f-c164-43c7acc2ef2e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517697730482,
          "user_tz": 300,
          "elapsed": 241,
          "user": {
            "displayName": "Zichao Yan",
            "photoUrl": "//lh3.googleusercontent.com/-lUGgNeiX7v8/AAAAAAAAAAI/AAAAAAAAABE/6cWBoWlDDyc/s50-c-k-no/photo.jpg",
            "userId": "105794893504586740188"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "learner = GridPolicyLearner(GridMDP())\n",
        "print('Policy Iteration for Gridworld')\n",
        "learner.PolicyIteration()\n",
        "print('\\n')\n",
        "print('Value Iteration for Gridworld')\n",
        "learner.ValueIteration()\n",
        "print('\\n')\n",
        "print('Modified Value Iteration for Gridworld')\n",
        "learner.ImprovedPolicyIteration(partial_iteration_limit=50)\n",
        "print('\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy Iteration for Gridworld\n",
            "Sweep count: 221\n",
            "value function:\n",
            "0.00\t-7.59\t-10.53\t-11.43\n",
            "-7.59\t-9.63\t-10.58\t-10.53\n",
            "-10.53\t-10.58\t-9.63\t-7.59\n",
            "-11.43\t-10.53\t-7.59\t0.00\n",
            "policy update: (state 1 -> action 3)\n",
            "policy update: (state 2 -> action 3)\n",
            "policy update: (state 3 -> action 2)\n",
            "policy update: (state 6 -> action 2)\n",
            "policy update: (state 7 -> action 2)\n",
            "policy update: (state 10 -> action 1)\n",
            "policy update: (state 11 -> action 2)\n",
            "policy update: (state 13 -> action 1)\n",
            "policy update: (state 14 -> action 1)\n",
            "Sweep count: 4\n",
            "value function:\n",
            "0.00\t-1.00\t-1.95\t-2.85\n",
            "-1.00\t-1.95\t-2.85\t-1.95\n",
            "-1.95\t-2.85\t-1.95\t-1.00\n",
            "-2.85\t-1.95\t-1.00\t0.00\n",
            "policy update: (state 6 -> action 0)\n",
            "Sweep count: 1\n",
            "value function:\n",
            "0.00\t-1.00\t-1.95\t-2.85\n",
            "-1.00\t-1.95\t-2.85\t-1.95\n",
            "-1.95\t-2.85\t-1.95\t-1.00\n",
            "-2.85\t-1.95\t-1.00\t0.00\n",
            "Itertion finished\n",
            "policy:\n",
            "terminal     left         left         down        \n",
            "top          top          top          down        \n",
            "top          top          right        down        \n",
            "top          right        right        terminal    \n",
            "\n",
            "\n",
            "Value Iteration for Gridworld\n",
            "Sweep count: 4\n",
            "value function:\n",
            "0.00\t-1.00\t-1.95\t-2.85\n",
            "-1.00\t-1.95\t-2.85\t-1.95\n",
            "-1.95\t-2.85\t-1.95\t-1.00\n",
            "-2.85\t-1.95\t-1.00\t0.00\n",
            "policy:\n",
            "terminal     left         left         down        \n",
            "top          top          top          down        \n",
            "top          top          right        down        \n",
            "top          right        right        terminal    \n",
            "\n",
            "\n",
            "Modified Value Iteration for Gridworld\n",
            "Sweep count: 50\n",
            "value function:\n",
            "0.00\t-7.56\t-10.48\t-11.37\n",
            "-7.56\t-9.58\t-10.52\t-10.48\n",
            "-10.48\t-10.52\t-9.58\t-7.56\n",
            "-11.37\t-10.48\t-7.56\t0.00\n",
            "policy update: (state 1 -> action 3)\n",
            "policy update: (state 2 -> action 3)\n",
            "policy update: (state 3 -> action 2)\n",
            "policy update: (state 6 -> action 2)\n",
            "policy update: (state 7 -> action 2)\n",
            "policy update: (state 10 -> action 1)\n",
            "policy update: (state 11 -> action 2)\n",
            "policy update: (state 12 -> action 1)\n",
            "policy update: (state 13 -> action 1)\n",
            "policy update: (state 14 -> action 1)\n",
            "Sweep count: 4\n",
            "value function:\n",
            "0.00\t-1.00\t-1.95\t-2.85\n",
            "-1.00\t-1.95\t-2.85\t-1.95\n",
            "-1.95\t-2.85\t-1.95\t-1.00\n",
            "-2.85\t-1.95\t-1.00\t0.00\n",
            "policy update: (state 6 -> action 0)\n",
            "policy update: (state 12 -> action 0)\n",
            "Sweep count: 1\n",
            "value function:\n",
            "0.00\t-1.00\t-1.95\t-2.85\n",
            "-1.00\t-1.95\t-2.85\t-1.95\n",
            "-1.95\t-2.85\t-1.95\t-1.00\n",
            "-2.85\t-1.95\t-1.00\t0.00\n",
            "Itertion finished\n",
            "policy:\n",
            "terminal     left         left         down        \n",
            "top          top          top          down        \n",
            "top          top          right        down        \n",
            "top          right        right        terminal    \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IZhe-c2vkDH9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "\n",
        "# Disable\n",
        "def blockPrint():\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "# Restore\n",
        "def enablePrint():\n",
        "    sys.stdout = sys.__stdout__\n",
        "    \n",
        "    \n",
        "def _test_gridworld(discount, suppress_print=False):\n",
        "    '''\n",
        "\n",
        "    :param discount:\n",
        "    :return: convergence sweep counts for each of the 3 algorithms\n",
        "    '''\n",
        "    if suppress_print:\n",
        "        blockPrint()\n",
        "    else:\n",
        "        enablePrint()\n",
        "    print(discount)\n",
        "    conv_count = []\n",
        "    learner = GridPolicyLearner(GridMDP(),discount=discount)\n",
        "    print('Policy Iteration for Gridworld')\n",
        "    conv_count.append(learner.PolicyIteration())\n",
        "    print('\\n')\n",
        "    print('Value Iteration for Gridworld')\n",
        "    conv_count.append(learner.ValueIteration())\n",
        "    print('\\n')\n",
        "    print('Modified Value Iteration for Gridworld')\n",
        "    conv_count.append(learner.ImprovedPolicyIteration(partial_iteration_limit=50))\n",
        "    print('\\n')\n",
        "    return np.array(conv_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mfTjNqmpoIk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Demonstrating empirical results for convergence with different discount factor."
      ]
    },
    {
      "metadata": {
        "id": "i4VqbWOb7jcZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "ab8d028c-debb-4488-b8b1-33a79a715fcd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517697733353,
          "user_tz": 300,
          "elapsed": 620,
          "user": {
            "displayName": "Zichao Yan",
            "photoUrl": "//lh3.googleusercontent.com/-lUGgNeiX7v8/AAAAAAAAAAI/AAAAAAAAABE/6cWBoWlDDyc/s50-c-k-no/photo.jpg",
            "userId": "105794893504586740188"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "legend = ['Policy Iteration', 'Value Iteration', 'Improved Policy Iteration']\n",
        "print(legend)\n",
        "count = []\n",
        "for discount_factor in np.arange(0,1.05,0.05):\n",
        "    count.append(_test_gridworld(discount_factor, suppress_print=True))\n",
        "count = np.array(count)\n",
        "# print(count)\n",
        "plt.figure()\n",
        "plt.ylabel('sweep counts')\n",
        "plt.xlabel('discount factor')\n",
        "for i in range(3):\n",
        "    plt.plot(np.arange(0,1.05,0.05),count[:,i], label=legend[i])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Policy Iteration', 'Value Iteration', 'Improved Policy Iteration']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFYCAYAAACoFn5YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7UYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4216a75588>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bOt40LRqreQ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the graph we can see that the convergence rate of value iteration is constant (4 sweeps) notwithstanding the discount factor; in comparison, policy iteration algorithm is exponentially proportional to the discount factor. Improved value iteration is moderate with respect to discount factor, whose maximum iteration per round is set to 50. The sweep counts of this algorithm remains constant after the discount factor go over a certain threshold."
      ]
    }
  ]
}